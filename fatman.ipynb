{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "\n",
    "\n",
    "spark_version = pyspark.__version__\n",
    "if not spark_version.startswith(\"3\"):\n",
    "    raise EnvironmentError(\n",
    "        f\"Can only execute this notebook on the kernel with Spark 3+ installed | Found: {spark_version}\"\n",
    "    )\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/PyPIconfig.sh\n",
    "!pip install mlopstools==1.0.37\n",
    "!pip install pyspark==3.1.1\n",
    "# Potentially need to restart kernel after executing this cell to make package available on kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import boto3\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import AggregateMessages as AM  # noqa\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql import types as st\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "from inference.src.settings import MIN_TRX_DATE, MAX_TRX_DATE, get_logger\n",
    "from inference.jobs.utils import setup_job\n",
    "\n",
    "\n",
    "if pyspark.__version__ != \"3.1.1\":\n",
    "    raise EnvironmentError(\"PySpark 3.1.1 not yet loaded | Please restart the kernel\")\n",
    "\n",
    "LOGGER = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Spark session setup\n",
    "\n",
    "_, spark = setup_job(\"exploration\", str(uuid.uuid4()), str(uuid.uuid4()))\n",
    "\n",
    "region = \"eu-west-1\"\n",
    "\n",
    "sts_client = boto3.client(\"sts\", region_name=region, endpoint_url=f\"https://sts.{region}.amazonaws.com\")\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "domain_id = sagemaker_client.list_domains()[\"Domains\"][0][\"DomainId\"]\n",
    "execution_role_arn = sagemaker_client.describe_domain(DomainId=domain_id)[\"DefaultUserSettings\"][\"ExecutionRole\"]\n",
    "credentials = sts_client.assume_role(RoleArn=execution_role_arn, RoleSessionName=\"sagemaker-pyspark\")\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(  # noqa\n",
    "    \"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\"\n",
    ")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(  # noqa\n",
    "    \"fs.s3a.access.key\", credentials[\"Credentials\"][\"AccessKeyId\"]\n",
    ")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(  # noqa\n",
    "    \"fs.s3a.secret.key\", credentials[\"Credentials\"][\"SecretAccessKey\"]\n",
    ")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(  # noqa\n",
    "    \"fs.s3a.session.token\", credentials[\"Credentials\"][\"SessionToken\"]\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BUCKET = \"tmnl-prod-data-scientist-sagemaker-data-intermediate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-input\"\n",
    "location_s3 = \"s3\" + location.lstrip(\"s3a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "available_dates = !aws s3 ls {location_s3}/\n",
    "available_dates = sorted(\n",
    "    set([x.strip().replace(\"PRE transaction_date=\", \"\").replace(\"/\", \"\")[:10] for x in available_dates])\n",
    ")[:-1]\n",
    "LOGGER.info(len(available_dates), available_dates[0], available_dates[-1])\n",
    "available_dates = set(available_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HORIZON = 21  # days\n",
    "MAX_CENTRALITY = 1000\n",
    "MIN_TOTAL_TRANSACTION_AMOUNT = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_data(dataframe):\n",
    "    window_source = Window.partitionBy(\"source\")\n",
    "    window_target = Window.partitionBy(\"target\")\n",
    "    return (\n",
    "        dataframe.withColumn(\"total_out\", sf.sum(\"amount\").over(window_source))\n",
    "        .withColumn(\"total_in\", sf.sum(\"amount\").over(window_target))\n",
    "        .withColumn(\"count_out\", sf.count(\"source\").over(window_source))\n",
    "        .withColumn(\"count_in\", sf.count(\"target\").over(window_target))\n",
    "        .where(\n",
    "            (\n",
    "                (sf.col(\"total_out\") >= MIN_TOTAL_TRANSACTION_AMOUNT)\n",
    "                | (sf.col(\"total_in\") >= MIN_TOTAL_TRANSACTION_AMOUNT)\n",
    "            )\n",
    "            & (sf.col(\"count_out\") < MAX_CENTRALITY)\n",
    "            & (sf.col(\"count_in\") < MAX_CENTRALITY)\n",
    "        )\n",
    "        .drop(\"total_out\", \"total_in\", \"count_out\", \"count_in\")\n",
    "    )\n",
    "\n",
    "\n",
    "def rename_columns(dataframe, names):\n",
    "    for name, new_name in names.items():\n",
    "        dataframe = dataframe.withColumnRenamed(name, new_name)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def max_timestamp(dt):\n",
    "    year, month, date = dt.split(\"-\")\n",
    "    return (datetime(int(year), int(month), int(date)) + timedelta(days=1)).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "left_columns = {x.name: f\"{x.name}_left\" for x in spark.read.parquet(location).schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "location_output = f\"s3a://{BUCKET}/community-detection/exploration/ftm-joins/\"\n",
    "dates = [str(x.date()) for x in sorted(pd.date_range(MIN_TRX_DATE, MAX_TRX_DATE))]\n",
    "for transaction_date in dates:\n",
    "    if not ({transaction_date}.intersection(available_dates)):\n",
    "        raise Exception(f\"[NotAvailable] {transaction_date}\")\n",
    "    start_index = dates.index(transaction_date)\n",
    "    end_index = start_index + HORIZON + 1\n",
    "    right_dates = dates[start_index:end_index]\n",
    "    right_dates = sorted(available_dates.intersection(right_dates))\n",
    "    if not right_dates:\n",
    "        raise Exception(f\"[NotAvailable] {dates[start_index:end_index]}\")\n",
    "    right_location = [f\"{location}/transaction_date={x}\" for x in right_dates]\n",
    "    right = filter_data(spark.read.parquet(*right_location)).cache()\n",
    "    _ = right.count()\n",
    "    left = rename_columns(right.where(right.transaction_timestamp < max_timestamp(transaction_date)), left_columns)\n",
    "    join = left.join(right, left.target_left == right.source, \"inner\")\n",
    "    join = join.withColumn(\"delta\", join.transaction_timestamp - join.transaction_timestamp_left)\n",
    "    join = join.where(join.delta > -1)\n",
    "    join.write.parquet(f\"{location_output}date={transaction_date}\", mode=\"overwrite\")\n",
    "    LOGGER.info(f\"[{transaction_date}] Ran in {timedelta(seconds=round(time.time() - start_time))}\")\n",
    "    start_time = time.time()\n",
    "    right.unpersist()\n",
    "    del right\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.parquet(location_output)\n",
    "\n",
    "# TODO: This is to avoid cycles in rare instances -> Implement a better solution\n",
    "data = data.where(data.transaction_timestamp > data.transaction_timestamp_left)\n",
    "\n",
    "data = data.withColumnRenamed(\"date\", \"transaction_date_left\").withColumn(\n",
    "    \"transaction_date\", sf.from_unixtime(\"transaction_timestamp\").cast(st.DateType())\n",
    ")\n",
    "\n",
    "location_nodes_1 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-node-1\"\n",
    "location_nodes_2 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-node-2\"\n",
    "node_columns = [\"id\", \"source\", \"target\", \"transaction_date\", \"transaction_timestamp\", \"amount\"]\n",
    "nodes_1 = (\n",
    "    (\n",
    "        data.select(\n",
    "            sf.col(\"id_left\").alias(\"id\"),\n",
    "            sf.col(\"source_left\").alias(\"source\"),\n",
    "            sf.col(\"target_left\").alias(\"target\"),\n",
    "            sf.col(\"transaction_timestamp_left\").alias(\"transaction_timestamp\"),\n",
    "            sf.col(\"amount_left\").alias(\"amount\"),\n",
    "            sf.col(\"transaction_date_left\").alias(\"transaction_date\"),\n",
    "        )\n",
    "    )\n",
    "    .select(*node_columns)\n",
    "    .drop_duplicates(subset=[\"id\"])\n",
    ")\n",
    "nodes_1.write.mode(\"overwrite\").parquet(location_nodes_1)\n",
    "nodes_2 = data.select(*node_columns).drop_duplicates(subset=[\"id\"])\n",
    "nodes_2.write.mode(\"overwrite\").parquet(location_nodes_2)\n",
    "nodes_1 = spark.read.parquet(location_nodes_1)\n",
    "nodes_2 = spark.read.parquet(location_nodes_2)\n",
    "nodes = nodes_1.union(nodes_2).drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "edges = data.select(\n",
    "    sf.col(\"id_left\").alias(\"src\"),\n",
    "    sf.col(\"id\").alias(\"dst\"),\n",
    "    sf.col(\"transaction_date_left\").alias(\"src_date\"),\n",
    "    sf.col(\"transaction_date\").alias(\"dst_date\"),\n",
    "    \"delta\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nodes_location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-nodes/\"\n",
    "edges_location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-edges/\"\n",
    "\n",
    "nodes.repartition(\"transaction_date\").write.partitionBy(\"transaction_date\").mode(\"overwrite\").parquet(nodes_location)\n",
    "partition_by = [\"src_date\", \"dst_date\"]\n",
    "edges.repartition(*partition_by).write.partitionBy(*partition_by).mode(\"overwrite\").parquet(edges_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pattern_for(hops):\n",
    "    last = \"x0\"\n",
    "    pattern_constructed = \"\"\n",
    "    for x in range(hops):\n",
    "        current = f\"x{x + 1}\"\n",
    "        edge = f\"e{x}\"\n",
    "        pattern_constructed += f\"({last}) - [{edge}] -> ({current}); \"\n",
    "        last = str(current)\n",
    "    return pattern_constructed.strip(\" ;\")\n",
    "\n",
    "\n",
    "def select_columns(hops, result):\n",
    "    columns = [sf.col(f\"x{x}.account\").alias(f\"x{x}\") for x in range(hops)]\n",
    "    return result.select(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This is important | Without setting the checkpoint directory, GraphFrames will fail\n",
    "spark.sparkContext.setCheckpointDir(\".\")\n",
    "\n",
    "nodes = spark.read.parquet(nodes_location)\n",
    "edges = spark.read.parquet(edges_location)\n",
    "\n",
    "graph = GraphFrame(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HOPS = 5\n",
    "\n",
    "pattern = pattern_for(HOPS)\n",
    "results = select_columns(HOPS, graph.find(pattern)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "array_sum = sf.udf(lambda x: int(np.array(x, dtype=int).sum()), st.IntegerType())\n",
    "pattern = \"(x1) - [e1] -> (x2)\"\n",
    "number_of_days_allowed = 7 * 24 * 60 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Type 1 flows\n",
    "\n",
    "results_t1 = (\n",
    "    graph.find(pattern)\n",
    "    .select(\"x1\", \"x2\", \"e1.delta\")\n",
    "    .where(sf.col(\"e1.delta\") < number_of_days_allowed)\n",
    "    .drop(\"e1.delta\")\n",
    "    .groupby(\"x2.source\")\n",
    "    .agg(\n",
    "        sf.collect_set(\"x1\").alias(\"left\"),\n",
    "        sf.collect_set(\"x2\").alias(\"right\"),\n",
    "        sf.countDistinct(\"x1.source\").alias(\"sources\"),\n",
    "        sf.countDistinct(\"x2.target\").alias(\"targets\"),\n",
    "    )\n",
    "    .where((sf.col(\"sources\") > 2) & (sf.col(\"targets\") > 2))\n",
    "    .withColumn(\"total\", array_sum(sf.col(\"right.amount\")))\n",
    "    .where(sf.col(\"total\") > 10000)\n",
    "    .withColumnRenamed(\"x2.source\", \"middle\")\n",
    "    .withColumn(\"start_transactions\", sf.size(\"left\"))\n",
    "    .withColumn(\"end_transactions\", sf.size(\"right\"))\n",
    ")\n",
    "location_t1 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-results-type-1\"\n",
    "results_t1.write.mode(\"overwrite\").parquet(location_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Type 2 flows\n",
    "\n",
    "results_t2 = (\n",
    "    graph.find(pattern)\n",
    "    .select(\"x1\", \"x2\", \"e1.delta\")\n",
    "    .where(sf.col(\"e1.delta\") < number_of_days_allowed)\n",
    "    .drop(\"e1.delta\")\n",
    "    .groupby(\"x1.source\", \"x2.target\")\n",
    "    .agg(\n",
    "        sf.collect_set(\"x1\").alias(\"left\"),\n",
    "        sf.collect_set(\"x2\").alias(\"right\"),\n",
    "        sf.countDistinct(\"x1.target\").alias(\"intermediaries\"),\n",
    "    )\n",
    "    .where(sf.col(\"intermediaries\") > 2)\n",
    "    .withColumn(\"total\", array_sum(sf.col(\"right.amount\")))\n",
    "    .where(sf.col(\"total\") > 10000)\n",
    "    .withColumn(\"start_transactions\", sf.size(\"left\"))\n",
    "    .withColumn(\"end_transactions\", sf.size(\"right\"))\n",
    ")\n",
    "location_t2 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-results-type-2\"\n",
    "results_t2.write.mode(\"overwrite\").parquet(location_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_t2 = results_t2.withColumn(\"percentage_transferred\", results_t2.total / array_sum(\"left.amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3-hop networks\n",
    "\n",
    "pattern = \"(x1) - [e1] -> (x2); (x2) - [e2] -> (x3)\"\n",
    "\n",
    "results_h3 = graph.find(pattern)\n",
    "result_h3 = results_h3.where((results_h3.e1.delta + results_h3.e2.delta) < number_of_days_allowed).select(\n",
    "    sf.array(\n",
    "        sf.concat(sf.lit(\"s-\"), \"x1.source\"),\n",
    "        sf.concat(sf.lit(\"m-\"), \"x1.target\"),\n",
    "        sf.concat(sf.lit(\"m-\"), \"x2.target\"),\n",
    "        sf.concat(sf.lit(\"t-\"), \"x3.target\"),\n",
    "    ).alias(\"items\")\n",
    ")\n",
    "location_h3 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-results-3-hops-input\"\n",
    "result_h3.write.mode(\"overwrite\").parquet(location_h3)\n",
    "\n",
    "fp = FPGrowth(minSupport=0.000001, minConfidence=0.5)\n",
    "fpm = fp.fit(result_h3)\n",
    "location_model = f\"s3a://{BUCKET}/community-detection/exploration/ftm-fpm-model\"\n",
    "fpm.write().overwrite().save(location_model)\n",
    "\n",
    "rules = fpm.associationRules.sort(\"antecedent\", \"consequent\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "location_fpm_t0 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-input-fpm-t0\"\n",
    "location_fpm_t1 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-input-fpm-t1\"\n",
    "location_fpm_t2 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-input-fpm-t2\"\n",
    "\n",
    "l0 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-input-fpm-pruned-t0\"\n",
    "l1 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-input-fpm-pruned-t1\"\n",
    "l2 = f\"s3a://{BUCKET}/community-detection/exploration/ftm-input-fpm-pruned-t2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FPM All Connections\n",
    "\n",
    "message = sf.array(\n",
    "    sf.concat(AM.src[\"source\"], sf.lit(\"-\"), AM.src[\"target\"]),\n",
    "    sf.concat(AM.src[\"target\"], sf.lit(\"-\"), AM.dst[\"target\"]),\n",
    ")\n",
    "t0 = (\n",
    "    graph.aggregateMessages(sf.collect_set(AM.msg).alias(\"items\"), sendToDst=message)\n",
    "    .select(\"id\", \"items\")\n",
    "    .repartition(1024, \"id\")\n",
    "    .cache()\n",
    ")\n",
    "LOGGER.info(f\"{t0.count():,} nodes processed\")\n",
    "\n",
    "schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"left\", st.StringType(), nullable=False),\n",
    "        st.StructField(\"right\", st.StringType(), nullable=False),\n",
    "        st.StructField(\"degree\", st.IntegerType(), nullable=False),\n",
    "        st.StructField(\"id\", st.IntegerType(), nullable=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@sf.pandas_udf(schema, sf.PandasUDFType.GROUPED_MAP)\n",
    "def unpivot(input_data):\n",
    "    row = input_data.iloc[0]\n",
    "    result = pd.DataFrame(row[\"items\"].tolist(), columns=[\"left\", \"right\"])\n",
    "    result.loc[:, \"degree\"] = result.shape[0]\n",
    "    result.loc[:, \"id\"] = row[\"id\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "t0.groupby(\"id\").apply(unpivot).write.mode(\"overwrite\").parquet(location_fpm_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Edges to keep -> Top 20% most frequent\n",
    "\n",
    "schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"src_left\", st.StringType(), nullable=False),\n",
    "        st.StructField(\"dst_left\", st.StringType(), nullable=False),\n",
    "        st.StructField(\"src_right\", st.StringType(), nullable=False),\n",
    "        st.StructField(\"dst_right\", st.StringType(), nullable=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@sf.pandas_udf(schema, sf.PandasUDFType.GROUPED_MAP)\n",
    "def select_edges(input_data):\n",
    "    columns = [\"src_left\", \"dst_left\", \"src_right\", \"dst_right\"]\n",
    "    src_right, dst_right = input_data.iloc[0][\"right\"].split(\"-\")\n",
    "    result = pd.DataFrame.from_dict(Counter(input_data[\"left\"]), \"index\", columns=[\"percentile_rank\"]).reset_index()\n",
    "    result.loc[:, \"percentile_rank\"] = result.loc[:, \"percentile_rank\"].rank(pct=True)\n",
    "    result = result.loc[result.percentile_rank > 0.7999, :]\n",
    "    if result.empty:\n",
    "        return pd.DataFrame(columns=columns)\n",
    "    result.loc[:, \"src_right\"] = src_right\n",
    "    result.loc[:, \"dst_right\"] = dst_right\n",
    "    left_side = np.array(result.loc[:, \"index\"].str.split(\"-\").tolist())\n",
    "    result.loc[:, \"src_left\"] = left_side[:, 0]\n",
    "    result.loc[:, \"dst_left\"] = left_side[:, 1]\n",
    "    return result.loc[:, columns]\n",
    "\n",
    "\n",
    "t0 = spark.read.parquet(location_fpm_t0)\n",
    "t0.select(\"left\", \"right\").groupby(\"right\").apply(select_edges).write.mode(\"overwrite\").parquet(l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FPM \"At least\" 60% of the amount carried forward\n",
    "\n",
    "message = sf.array(\n",
    "    sf.concat(AM.src[\"source\"], sf.lit(\"-\"), AM.src[\"target\"]),\n",
    "    sf.concat(AM.src[\"target\"], sf.lit(\"-\"), AM.dst[\"target\"]),\n",
    "    AM.src[\"amount\"],\n",
    "    AM.dst[\"amount\"],\n",
    ")\n",
    "\n",
    "\n",
    "@sf.udf(st.ArrayType(st.ArrayType(st.StringType())))\n",
    "def filter_connections(rows):\n",
    "    forwarded = int(rows[0][3])\n",
    "    threshold = forwarded * 0.5999\n",
    "    connections = defaultdict(int)\n",
    "    for row in rows:\n",
    "        connections[row[0]] += int(row[2])\n",
    "    right_node = rows[0][1]\n",
    "    return [(k, right_node) for k, v in connections.items() if v > threshold]\n",
    "\n",
    "\n",
    "t1 = (\n",
    "    graph.aggregateMessages(sf.collect_list(AM.msg).alias(\"items_raw\"), sendToDst=message)\n",
    "    .withColumn(\"items\", filter_connections(sf.col(\"items_raw\")))\n",
    "    .select(\"id\", \"items\")\n",
    "    .where(sf.col(\"items\") != sf.array())\n",
    "    .repartition(1024, \"id\")\n",
    "    .cache()\n",
    ")\n",
    "LOGGER.info(f\"{t1.count():,} nodes processed\")\n",
    "\n",
    "t1.groupby(\"id\").apply(unpivot).write.mode(\"overwrite\").parquet(location_fpm_t1)\n",
    "\n",
    "t1 = spark.read.parquet(location_fpm_t1)\n",
    "t1.select(\"left\", \"right\").groupby(\"right\").apply(select_edges).write.mode(\"overwrite\").parquet(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FPM \"At most\" 40% of the amount carried forward\n",
    "\n",
    "\n",
    "@sf.udf(st.ArrayType(st.ArrayType(st.StringType())))\n",
    "def filter_connections(rows):\n",
    "    forwarded = int(rows[0][3])\n",
    "    threshold = forwarded * 0.4001\n",
    "    connections = defaultdict(int)\n",
    "    for row in rows:\n",
    "        connections[row[0]] += int(row[2])\n",
    "    right_node = rows[0][1]\n",
    "    return [(k, right_node) for k, v in connections.items() if v < threshold]\n",
    "\n",
    "\n",
    "t2 = (\n",
    "    graph.aggregateMessages(sf.collect_list(AM.msg).alias(\"items_raw\"), sendToDst=message)\n",
    "    .withColumn(\"items\", filter_connections(sf.col(\"items_raw\")))\n",
    "    .select(\"id\", \"items\")\n",
    "    .where(sf.col(\"items\") != sf.array())\n",
    "    .repartition(1024, \"id\")\n",
    "    .cache()\n",
    ")\n",
    "LOGGER.info(f\"{t2.count():,} nodes processed\")\n",
    "\n",
    "t2.groupby(\"id\").apply(unpivot).write.mode(\"overwrite\").parquet(location_fpm_t2)\n",
    "\n",
    "t2 = spark.read.parquet(location_fpm_t2)\n",
    "t2.select(\"left\", \"right\").groupby(\"right\").apply(select_edges).write.mode(\"overwrite\").parquet(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# `Connections` graph / community detection\n",
    "\n",
    "pattern = \"(x1) - [e1] -> (x2)\"\n",
    "nodes_days = HORIZON + 1\n",
    "for start_date in pd.date_range(\"2021-04-01\", freq=\"d\", periods=365):\n",
    "    start_date = start_date.date()\n",
    "    start_time = time.time()\n",
    "    nodes_dates = [str(x.date()) for x in sorted(pd.date_range(start_date, periods=nodes_days, freq=\"d\"))]\n",
    "    nodes_dates = [x for x in nodes_dates if x <= MIN_TRX_DATE]\n",
    "    nodes_locations = [f\"{nodes_location}transaction_date={x}/\" for x in nodes_dates]\n",
    "    day_edges = spark.read.parquet(f\"{edges_location}src_date={start_date}/\")\n",
    "    day_nodes = spark.read.parquet(*nodes_locations)\n",
    "    graph = GraphFrame(day_nodes, day_edges)\n",
    "    graph.find(pattern).select(\n",
    "        sf.col(\"x1.source\").alias(\"start\"),\n",
    "        sf.col(\"x1.target\").alias(\"middle\"),\n",
    "        sf.col(\"x2.target\").alias(\"end\"),\n",
    "    ).dropDuplicates().write.mode(\"overwrite\").parquet(\n",
    "        f\"s3a://{BUCKET}/community-detection/exploration/ftm-fpm-input/date={start_date}\"\n",
    "    )\n",
    "    LOGGER.info(f\"[{start_date}] Ran in {timedelta(seconds=round(time.time() - start_time))}\")\n",
    "\n",
    "schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"src\", st.StringType(), nullable=False),\n",
    "        st.StructField(\"dst\", st.StringType(), nullable=False),\n",
    "        st.StructField(\"weight\", st.FloatType(), nullable=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@sf.pandas_udf(schema, sf.PandasUDFType.GROUPED_MAP)\n",
    "def create_connection_edges(input_data):\n",
    "    end = input_data.iloc[0][\"end\"]\n",
    "    input_data = (\n",
    "        input_data.groupby(\"start\").agg({\"middle\": \"first\", \"end\": \"count\", \"dst_count_per_src\": \"first\"}).reset_index()\n",
    "    )\n",
    "    input_data.loc[:, \"weight\"] = input_data.loc[:, \"end\"] / input_data.loc[:, \"dst_count_per_src\"]\n",
    "    input_data.loc[:, \"src\"] = input_data.start + \"-\" + input_data.middle\n",
    "    input_data.loc[:, \"dst\"] = input_data.middle + f\"-{end}\"\n",
    "    return input_data.loc[:, [\"src\", \"dst\", \"weight\"]]\n",
    "\n",
    "\n",
    "edges_connections = spark.read.parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-fpm-input/\")\n",
    "dst_counts = edges_connections.groupby(\"start\", \"middle\").agg(sf.count(\"end\").alias(\"dst_count_per_src\")).cache()\n",
    "LOGGER.info(f\"{dst_counts.count():,} `sources` found\")\n",
    "dst_counts.write.mode(\"overwrite\").parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-temp-1\")\n",
    "\n",
    "edges_connections = (\n",
    "    dst_counts.alias(\"left\")\n",
    "    .join(\n",
    "        edges_connections.alias(\"right\"),\n",
    "        (dst_counts.start == edges_connections.start) & (dst_counts.middle == edges_connections.middle),\n",
    "        \"inner\",\n",
    "    )\n",
    "    .select(sf.col(\"left.start\"), sf.col(\"left.middle\"), sf.col(\"right.end\"), \"dst_count_per_src\")\n",
    "    .cache()\n",
    ")\n",
    "LOGGER.info(f\"{edges_connections.count():,} `edges` found\")\n",
    "edges_connections.write.mode(\"overwrite\").parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-temp-2\")\n",
    "\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-connection-edges\"\n",
    "edges_connections.groupby(\"middle\", \"end\").apply(create_connection_edges).write.mode(\"overwrite\").parquet(location)\n",
    "\n",
    "edges_connections = spark.read.parquet(location)\n",
    "LOGGER.info(f\"{edges_connections.count():,} `edges` left\")\n",
    "\n",
    "nodes_connections = (\n",
    "    edges_connections.select(sf.col(\"src\").alias(\"id\")).union(edges_connections.select(sf.col(\"dst\").alias(\"id\")))\n",
    ").dropDuplicates()\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-connection-nodes\"\n",
    "nodes_connections.write.mode(\"overwrite\").parquet(location)\n",
    "\n",
    "nodes_connections = spark.read.parquet(location)\n",
    "graph = GraphFrame(nodes_connections, edges_connections)\n",
    "\n",
    "communities = graph.labelPropagation(maxIter=1)\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-connection-communities\"\n",
    "communities.select(\"id\", \"label\").write.mode(\"overwrite\").parquet(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Processing a window\n",
    "\n",
    "start_date = \"2021-04-01\"\n",
    "edges_days = 21\n",
    "nodes_days = int(edges_days * 2)\n",
    "\n",
    "nodes_dates = [str(x.date()) for x in sorted(pd.date_range(start_date, periods=nodes_days, freq=\"d\"))]\n",
    "nodes_locations = [f\"{nodes_location}transaction_date={x}/\" for x in nodes_dates]\n",
    "\n",
    "edges_locations = []\n",
    "for src_date in [str(x.date()) for x in sorted(pd.date_range(start_date, periods=edges_days, freq=\"d\"))]:\n",
    "    dst_dates = [str(x.date()) for x in sorted(pd.date_range(src_date, periods=edges_days, freq=\"d\"))]\n",
    "    for dst_date in dst_dates:\n",
    "        edges_locations.append(f\"{edges_location}src_date={src_date}/dst_date={dst_date}\")\n",
    "\n",
    "nodes = spark.read.parquet(*nodes_locations)\n",
    "edges = spark.read.parquet(*edges_locations)\n",
    "\n",
    "graph = GraphFrame(nodes, edges)\n",
    "pattern = \"(x1) - [e1] -> (x2)\"\n",
    "\n",
    "connection_edges = spark.read.parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-connection-edges\")\n",
    "\n",
    "window_connections = (\n",
    "    graph.find(pattern)\n",
    "    .select(\n",
    "        sf.concat(sf.col(\"x1.source\"), sf.lit(\"-\"), sf.col(\"x1.target\")).alias(\"src\"),\n",
    "        sf.concat(sf.col(\"x2.source\"), sf.lit(\"-\"), sf.col(\"x2.target\")).alias(\"dst\"),\n",
    "    )\n",
    "    .dropDuplicates()\n",
    "    .cache()\n",
    ")\n",
    "LOGGER.info(f\"{window_connections.count():,} `connections` found\")\n",
    "\n",
    "widow_connection_edges = window_connections.join(connection_edges, [\"src\", \"dst\"], \"inner\")\n",
    "\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-connection-edges\"\n",
    "widow_connection_edges.write.mode(\"overwrite\").parquet(location)\n",
    "widow_connection_edges = spark.read.parquet(location)\n",
    "LOGGER.info(f\"{widow_connection_edges.count():,} `edges` found\")\n",
    "\n",
    "window_connection_nodes = (\n",
    "    widow_connection_edges.select(sf.col(\"src\").alias(\"id\")).union(\n",
    "        widow_connection_edges.select(sf.col(\"dst\").alias(\"id\"))\n",
    "    )\n",
    ").dropDuplicates()\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-connection-nodes\"\n",
    "window_connection_nodes.write.mode(\"overwrite\").parquet(location)\n",
    "\n",
    "LOGGER.info(\"Getting connected components\")\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/connected-components\"\n",
    "graph.connectedComponents().write.mode(\"overwrite\").parquet(location)\n",
    "\n",
    "LOGGER.info(\"Loading connected components\")\n",
    "cc = spark.read.parquet(location)\n",
    "count = cc.select(\"component\").distinct().count()\n",
    "LOGGER.info(f\"Found {count:,} connected components\")\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "\n",
    "MIN_WEIGHT = 0.000999\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for x in range(20):  # Correct this number based on number of files\n",
    "    loc = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-connection-edges/part-{f'{x}'.zfill(4)}*\"\n",
    "    chunk = spark.read.parquet(loc)\n",
    "    data = data.append(chunk.where(chunk.weight > MIN_WEIGHT).toPandas(), ignore_index=True)\n",
    "    LOGGER.info(f\"Processed chunk number {x} | {data.shape[0]:,}\")\n",
    "\n",
    "data = pd.read_parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-connection-edges/\")\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "graph = ig.Graph.DataFrame(data, use_vids=False, directed=True)\n",
    "LOGGER.info(\"Graph Loaded\")\n",
    "communities = la.find_partition(\n",
    "    graph, la.ModularityVertexPartition, weights=\"weight\", n_iterations=5, max_comm_size=100\n",
    ")\n",
    "LOGGER.info(\"Communities Detected\")\n",
    "communities_output = graph.get_vertex_dataframe()\n",
    "communities_output.loc[:, \"label\"] = communities.membership\n",
    "cluster_graph = communities.cluster_graph()\n",
    "mapping = dict(zip(cluster_graph.get_vertex_dataframe().index, cluster_graph.clusters().membership))\n",
    "communities_output.loc[:, \"label_cluster\"] = communities_output.loc[:, \"label\"].apply(mapping.get)\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-communities\"\n",
    "spark.createDataFrame(communities_output).write.mode(\"overwrite\").parquet(location)\n",
    "\n",
    "# TODO: Instead of using the weights (or weight filter), keep \"all\" edges b/w nodes\n",
    "# ...for the LPA community detection on Spark\n",
    "nodes = spark.read.parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-connection-nodes\")\n",
    "edges = spark.read.parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-connection-edges\")\n",
    "edges = edges.where(edges.weight > MIN_WEIGHT).cache()\n",
    "LOGGER.info(f\"Running LPA for {edges.count():,} edges\")\n",
    "graph = GraphFrame(nodes, edges)\n",
    "communities = graph.labelPropagation(maxIter=5)\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-lpa-communities\"\n",
    "communities.select(\"id\", \"label\").write.mode(\"overwrite\").parquet(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Combine `connection communities` with flows\n",
    "\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-lpa-communities\"\n",
    "communities = spark.read.parquet(location)\n",
    "\n",
    "nodes = spark.read.parquet(*nodes_locations)\n",
    "results = communities.join(\n",
    "    nodes.withColumnRenamed(\"id\", \"id_actual\"),\n",
    "    communities.id == sf.concat(sf.col(\"source\"), sf.lit(\"-\"), sf.col(\"target\")),\n",
    "    \"inner\",\n",
    ")\n",
    "results = results.drop(\"id\").withColumnRenamed(\"id_actual\", \"id\")\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-communities-m1\"\n",
    "results.write.mode(\"overwrite\").parquet(location)\n",
    "\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-communities\"\n",
    "communities = spark.read.parquet(location)\n",
    "results = communities.join(\n",
    "    nodes,\n",
    "    communities.name == sf.concat(sf.col(\"source\"), sf.lit(\"-\"), sf.col(\"target\")),\n",
    "    \"inner\",\n",
    ")\n",
    "results = results.drop(\"name\")\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-communities-m2\"\n",
    "results.write.mode(\"overwrite\").parquet(location)\n",
    "\n",
    "communities = spark.read.parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-communities-m2\")\n",
    "schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"label\", st.IntegerType(), nullable=False),\n",
    "        st.StructField(\"label_cluster\", st.IntegerType(), nullable=False),\n",
    "        st.StructField(\"dispensers\", st.IntegerType(), nullable=False),\n",
    "        st.StructField(\"intermediates\", st.IntegerType(), nullable=False),\n",
    "        st.StructField(\"sinks\", st.IntegerType(), nullable=False),\n",
    "        st.StructField(\"forwarded\", st.IntegerType(), nullable=False),\n",
    "        st.StructField(\"received\", st.IntegerType(), nullable=False),\n",
    "        st.StructField(\"percentage_forwarded\", st.FloatType(), nullable=False),\n",
    "        st.StructField(\"diameter\", st.IntegerType(), nullable=False),\n",
    "        st.StructField(\"components\", st.IntegerType(), nullable=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@sf.pandas_udf(schema, sf.PandasUDFType.GROUPED_MAP)\n",
    "def community_summary(input_data):\n",
    "    first = input_data.iloc[0].to_dict()\n",
    "    sources = set(input_data.loc[:, \"source\"])\n",
    "    targets = set(input_data.loc[:, \"target\"])\n",
    "    dispensers = sources.difference(targets)\n",
    "    intermediates = sources.intersection(targets)\n",
    "    sinks = targets.difference(sources)\n",
    "    result = pd.DataFrame([first[\"label\"]], columns=[\"label\"])\n",
    "    result.loc[:, \"label_cluster\"] = first[\"label_cluster\"]\n",
    "    result.loc[:, \"dispensers\"] = len(dispensers)\n",
    "    result.loc[:, \"intermediates\"] = len(intermediates)\n",
    "    result.loc[:, \"sinks\"] = len(sinks)\n",
    "    input_data.loc[:, \"is_dispenser\"] = input_data.loc[:, \"source\"].isin(dispensers)\n",
    "    input_data.loc[:, \"is_sink\"] = input_data.loc[:, \"target\"].isin(sinks)\n",
    "    result.loc[:, \"forwarded\"] = sum(input_data.loc[input_data[\"is_dispenser\"], \"amount\"])\n",
    "    result.loc[:, \"received\"] = sum(input_data.loc[input_data[\"is_sink\"], \"amount\"]) or 1\n",
    "    result.loc[:, \"percentage_forwarded\"] = result.loc[:, \"received\"] / result.loc[:, \"forwarded\"]\n",
    "    columns = [\"source\", \"target\", \"transaction_timestamp\", \"amount\"]\n",
    "    graph_flow = ig.Graph.DataFrame(input_data.loc[:, columns], use_vids=False, directed=True)\n",
    "    result.loc[:, \"diameter\"] = graph_flow.diameter()\n",
    "    result.loc[:, \"components\"] = len(graph_flow.clusters(mode=\"weak\").sizes())\n",
    "    return result\n",
    "\n",
    "\n",
    "window_communities_summary = communities.groupby(\"label\").apply(community_summary)\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-window-communities-summary-m2\"\n",
    "window_communities_summary.write.mode(\"overwrite\").parquet(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Pruning\n",
    "\n",
    "selected = spark.read.parquet(l0)\n",
    "\n",
    "pattern = \"(x1) - [e1] -> (x2)\"\n",
    "original = graph.find(pattern)\n",
    "join_on = (\n",
    "    (original.x1.source == selected.src_left)\n",
    "    & (original.x1.target == selected.dst_left)\n",
    "    & (original.x2.source == selected.src_right)\n",
    "    & (original.x2.target == selected.dst_right)\n",
    ")\n",
    "joined = original.join(selected, join_on, \"inner\").select(sf.col(\"e1.src\").alias(\"src\"), sf.col(\"e1.dst\").alias(\"dst\"))\n",
    "output_location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-t0-edges\"\n",
    "joined.write.mode(\"overwrite\").parquet(output_location)\n",
    "\n",
    "edges = spark.read.parquet(output_location)\n",
    "\n",
    "LOGGER.info(f\"{edges.count():,} edges selected!\")\n",
    "\n",
    "nodes = edges.select(sf.col(\"src\").alias(\"id\")).union(edges.select(sf.col(\"dst\").alias(\"id\"))).dropDuplicates()\n",
    "output_location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-t0-nodes\"\n",
    "nodes.write.mode(\"overwrite\").parquet(output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Community detection\n",
    "\n",
    "nodes = spark.read.parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-t0-nodes\")\n",
    "edges = spark.read.parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-t0-edges\")\n",
    "\n",
    "graph = GraphFrame(nodes, edges)\n",
    "\n",
    "communities = graph.labelPropagation(maxIter=1)\n",
    "output_location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-t0-communities\"\n",
    "communities.select(\"id\", \"label\").write.mode(\"overwrite\").parquet(output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Single node (all) flows\n",
    "\n",
    "node = \"x\"\n",
    "ids = nodes.where((nodes.source == node) | (nodes.target == node)).select(\"id\").toPandas()[\"id\"].tolist()\n",
    "LOGGER.info(f\"Selected nodes = {len(ids):,}\")\n",
    "pattern = \"(x0) - [e0] -> (x1); (x1) - [e1] -> (x2); (x2) - [e2] -> (x3); (x3) - [e3] -> (x4)\"\n",
    "for hop in range(5):\n",
    "    flows = graph.find(pattern).where(sf.col(f\"x{hop}.id\").isin(ids))\n",
    "    flows = flows.select(\n",
    "        flows.x0.source.alias(\"a\"),\n",
    "        flows.x0.target.alias(\"b\"),\n",
    "        flows.x1.target.alias(\"c\"),\n",
    "        flows.x2.target.alias(\"d\"),\n",
    "        flows.x3.target.alias(\"e\"),\n",
    "        flows.x4.target.alias(\"f\"),\n",
    "    ).dropDuplicates()\n",
    "    output_location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-single-node-flows/hop={hop}\"\n",
    "    flows.write.mode(\"overwrite\").parquet(output_location)\n",
    "    LOGGER.info(f\"Processed hop #{hop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Personalised PageRank\n",
    "\n",
    "nodes = spark.read.parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-connection-nodes\")\n",
    "edges = spark.read.parquet(f\"s3a://{BUCKET}/community-detection/exploration/ftm-connection-edges\")\n",
    "edges_reversed = edges.select(\n",
    "    edges.dst.alias(\"src\"),\n",
    "    edges.src.alias(\"dst\"),\n",
    ")\n",
    "\n",
    "node = \"x\"\n",
    "nodes = nodes.select(\"id\", sf.split(nodes.id, \"-\", 0).alias(\"ids\"))\n",
    "source_ids = nodes.where((nodes.ids.getItem(0) == node) | (nodes.ids.getItem(1) == node)).select(\"id\").toPandas()\n",
    "source_ids = source_ids[\"id\"].tolist()\n",
    "\n",
    "LOGGER.info(f\"`sources` count = {len(source_ids)}\")\n",
    "\n",
    "graph_forward = GraphFrame(nodes, edges)\n",
    "graph_backward = GraphFrame(nodes, edges_reversed)\n",
    "\n",
    "ppr_forward = graph_forward.parallelPersonalizedPageRank(resetProbability=0.15, sourceIds=source_ids, maxIter=1)\n",
    "ppr_forward = ppr_forward.where(ppr_forward.weight > 0.4)\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-ppr-forward\"\n",
    "ppr_forward.edges.select(\"src\", \"dst\", \"weight\").write.mode(\"overwrite\").parquet(location)\n",
    "\n",
    "ppr_backward = graph_backward.parallelPersonalizedPageRank(resetProbability=0.15, sourceIds=source_ids, maxIter=1)\n",
    "ppr_backward = ppr_backward.where(ppr_backward.weight > 0.4)\n",
    "location = f\"s3a://{BUCKET}/community-detection/exploration/ftm-ppr-backward\"\n",
    "ppr_backward.edges.select(\"src\", \"dst\", \"weight\").write.mode(\"overwrite\").parquet(location)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "python3 (datascience-image-coral-dev/0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}